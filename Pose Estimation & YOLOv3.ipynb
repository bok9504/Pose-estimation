{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어플 상 화면 : 카메라가 받은 FPS로 그대로 출력\n",
    "\n",
    "Pose Estimate : FPS 2 단위로 알로리즘 추출 후 특정 자세 추출시 촬영 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:53.486093Z",
     "start_time": "2020-11-18T11:31:53.037734Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:54.682928Z",
     "start_time": "2020-11-18T11:31:53.502124Z"
    }
   },
   "outputs": [],
   "source": [
    "# fashion_pose.py : MPII를 사용한 신체부위 검출\n",
    "\n",
    "\n",
    "# MPII에서 각 파트 번호, 선으로 연결될 POSE_PAIRS\n",
    "\n",
    "BODY_PARTS = { \"Head\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "                \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"Chest\": 14,\n",
    "                \"Background\": 15 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Head\", \"Neck\"], [\"Neck\", \"RShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "                [\"RElbow\", \"RWrist\"], [\"Neck\", \"LShoulder\"], [\"LShoulder\", \"LElbow\"],\n",
    "                [\"LElbow\", \"LWrist\"], [\"Neck\", \"Chest\"], [\"Chest\", \"RHip\"], [\"RHip\", \"RKnee\"],\n",
    "                [\"RKnee\", \"RAnkle\"], [\"Chest\", \"LHip\"], [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"] ]\n",
    "    \n",
    "# 각 파일 path\n",
    "protoFile = \"model&weight/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "weightsFile = \"model&weight/pose/mpi/pose_iter_160000.caffemodel\"\n",
    "\n",
    " \n",
    "# 위의 path에 있는 network 불러오기\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:54.712955Z",
     "start_time": "2020-11-18T11:31:54.700954Z"
    }
   },
   "outputs": [],
   "source": [
    "def pose_estimate(frame):\n",
    "\n",
    "    # frame.shape = 불러온 이미지에서 height, width, color 받아옴\n",
    "    inWidth = 368\n",
    "    inHeight = 368\n",
    "    frameHeight, frameWidth, _ = frame.shape\n",
    "\n",
    "    # network에 넣기위해 전처리\n",
    "    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    # network에 넣어주기\n",
    "    net.setInput(inpBlob)\n",
    "\n",
    "    # 결과 받아오기\n",
    "    output = net.forward()\n",
    "\n",
    "    # output.shape[0] = 이미지 ID, [1] = 출력 맵의 높이, [2] = 너비\n",
    "    H = output.shape[2]\n",
    "    W = output.shape[3]\n",
    "\n",
    "    # 키포인트 검출시 이미지에 그려줌\n",
    "    points = []\n",
    "    for i in range(0,15):\n",
    "        # 해당 신체부위 신뢰도 얻음.\n",
    "        probMap = output[0, i, :, :]\n",
    "\n",
    "        # global 최대값 찾기\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "        # 원래 이미지에 맞게 점 위치 변경\n",
    "        x = (frameWidth * point[0]) / W\n",
    "        y = (frameHeight * point[1]) / H\n",
    "\n",
    "        # 키포인트 검출한 결과가 0.1보다 크면(검출한곳이 위 BODY_PARTS랑 맞는 부위면) points에 추가, 검출했는데 부위가 없으면 None으로    \n",
    "        if prob > 0.1 :    \n",
    "            cv2.circle(frame, (int(x), int(y)), 3, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)       # circle(그릴곳, 원의 중심, 반지름, 색)\n",
    "            cv2.putText(frame, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, lineType=cv2.LINE_AA)\n",
    "            points.append((int(x), int(y)))\n",
    "        else :\n",
    "            points.append(None)\n",
    "\n",
    "    # 이미지 복사\n",
    "    frameCopy = frame\n",
    "\n",
    "    # 각 POSE_PAIRS별로 선 그어줌 (머리 - 목, 목 - 왼쪽어깨, ...)\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]             # Head\n",
    "        partA = BODY_PARTS[partA]   # 0\n",
    "        partB = pair[1]             # Neck\n",
    "        partB = BODY_PARTS[partB]   # 1\n",
    "\n",
    "        #print(partA,\" 와 \", partB, \" 연결\\n\")\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(frameCopy, points[partA], points[partB], (0, 255, 0), 2)\n",
    "            \n",
    "    return frameCopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:54.742946Z",
     "start_time": "2020-11-18T11:31:54.728944Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 0.0\n"
     ]
    }
   ],
   "source": [
    "# FPS 측정\n",
    "start = time.time()\n",
    "print('time :', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:54.772933Z",
     "start_time": "2020-11-18T11:31:54.758968Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 웹캠를 구동하여 pose estimation을 수행하는 함수\n",
    "\n",
    "def showWebcam(FPS):\n",
    "\n",
    "    try:\n",
    "        print('카메라를 구동합니다.')\n",
    "        cap = cv2.VideoCapture(0) # VideoCapture 객체 생성 -> 구동할 장치 번호 입력(캠이 하나임으로 0번)\n",
    "    except:                      # 만약 캠구동이 아니라 저장된 파일을 재생하기 원할 시, 경로와 파일이름 입력\n",
    "        print('카메라 구동 실패')\n",
    "        return\n",
    "\n",
    "    cap.set(3, 480) # 프레임 크기 설정\n",
    "    cap.set(4, 320)    \n",
    "    prev_time = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read() # 재생되는 비디오를 한 프레임씩 읽기 \n",
    "        frame = cv2.flip(frame, 1) # 제대로 읽을 시에 ret : True or False, frame : 읽은 프레임\n",
    "        current_time = time.time() - prev_time\n",
    "        \n",
    "        if not ret:\n",
    "            print('비디오 읽기 오류')\n",
    "            break\n",
    "        \n",
    "        framePose = pose_estimate(frame)\n",
    "        \n",
    "        if (ret is True) and (current_time > 1./ FPS) :\n",
    "            prev_time = time.time()\n",
    "            cv2.imshow('video', framePose) # 변환한 프레임을 화면에 디스플레이\n",
    "        \n",
    "        \n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            print('카메라 구동을 종료합니다.')\n",
    "            break\n",
    "    \n",
    "    cap.release() # 오픈한 cap 객체 해제 **** 필수 ****\n",
    "    cv2.destroyAllWindows() # 윈도우 창 답기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:55.284650Z",
     "start_time": "2020-11-18T11:31:55.268621Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 저장된 이미지에 pose estimation을 수행하는 함수\n",
    "\n",
    "def imshow(images):\n",
    "    try:\n",
    "        frame = cv2.imread(images)\n",
    "        framePose = pose_estimate(frame)\n",
    "        cv2.imshow(images, framePose)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    except:\n",
    "        print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:31:55.618953Z",
     "start_time": "2020-11-18T11:31:55.613993Z"
    }
   },
   "outputs": [],
   "source": [
    "# 저장된 비디오에 pose estimation을 수행하는 함수\n",
    "\n",
    "def showVideo(input_source):\n",
    "    cap = cv2.VideoCapture(input_source)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read() # 재생되는 비디오를 한 프레임씩 읽기 \n",
    "        frame = cv2.flip(frame, 1) # 제대로 읽을 시에 ret : True or False, frame : 읽은 프레임\n",
    "\n",
    "        if not ret:\n",
    "            print('비디오 읽기 오류')\n",
    "            break\n",
    "\n",
    "        framePose = pose_estimate(frame)\n",
    "\n",
    "        cv2.imshow('video', framePose) # 변환한 프레임을 화면에 디스플레이\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == 27: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:15:58.919593Z",
     "start_time": "2020-11-18T11:15:32.084013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라를 구동합니다.\n",
      "카메라 구동을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "showWebcam(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T12:40:58.342915Z",
     "start_time": "2020-11-18T12:40:52.506156Z"
    }
   },
   "outputs": [],
   "source": [
    "#imshow('../2.data/0.dataset/crossedArm/istockphoto-1092806334-1024x1024.jpg')\n",
    "imshow('../2.data/0.dataset/crossedArm/crossedArm5.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T14:47:17.297023Z",
     "start_time": "2020-11-16T14:47:10.548055Z"
    }
   },
   "outputs": [],
   "source": [
    "showVideo('../2.data/0.dataset/test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T11:43:37.914291Z",
     "start_time": "2020-11-16T11:42:54.975469Z"
    }
   },
   "outputs": [],
   "source": [
    "# pose_estimation 결과물 생성 및 저장\n",
    "path = '../2.data/0.dataset/'\n",
    "path_list = ['crossedArm','handsup','meditation','Siuuuu']\n",
    "\n",
    "for i in path_list:\n",
    "    oslist = os.listdir(path + i)\n",
    "    for j in oslist:\n",
    "        frame = cv2.imread(path + i + '/' + j)\n",
    "        framePose = pose_estimate(frame)\n",
    "        cv2.imwrite(path + 'result/' + j,frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T11:31:55.025148Z",
     "start_time": "2020-11-16T11:31:54.979181Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../2.data/0.dataset/handsup/103964500-studio-shot-of-a-young-woman-in-a-bikini-isolated-on-white-having-her-hands-up-in-the-air-like-prese.jpg => ../2.data/0.dataset/handsup/handsup1.jpg\n",
      "../2.data/0.dataset/handsup/56924852-young-woman-raising-hands-in-hawaii-ready-to-go-snorkeling.jpg => ../2.data/0.dataset/handsup/handsup2.jpg\n",
      "../2.data/0.dataset/handsup/657326.jpg => ../2.data/0.dataset/handsup/handsup3.jpg\n",
      "../2.data/0.dataset/handsup/83755524-women-raise-her-both-hands-up.jpg => ../2.data/0.dataset/handsup/handsup4.jpg\n",
      "../2.data/0.dataset/handsup/8958613_stock-photo-casual-man-standing-with-hands-in-the-air-pointing-up.jpg => ../2.data/0.dataset/handsup/handsup5.jpg\n",
      "../2.data/0.dataset/handsup/a-young-happy-woman-stands-with-her-hands-in-the-air-XDY0AG.jpg => ../2.data/0.dataset/handsup/handsup6.jpg\n",
      "../2.data/0.dataset/handsup/a20723a42e454bcdc77a7ddae7b5510e.jpg => ../2.data/0.dataset/handsup/handsup7.jpg\n",
      "../2.data/0.dataset/handsup/depositphotos_70489547-stock-photo-cheerful-man-with-hands-up (1).jpg => ../2.data/0.dataset/handsup/handsup8.jpg\n",
      "../2.data/0.dataset/handsup/depositphotos_70489547-stock-photo-cheerful-man-with-hands-up.jpg => ../2.data/0.dataset/handsup/handsup9.jpg\n",
      "../2.data/0.dataset/handsup/depositphotos_96653048-stock-photo-woman-in-bikini-with-hands.jpg => ../2.data/0.dataset/handsup/handsup10.jpg\n",
      "../2.data/0.dataset/handsup/hands-up-8821200.jpg => ../2.data/0.dataset/handsup/handsup11.jpg\n",
      "../2.data/0.dataset/handsup/happy-man-with-his-hands-raised-up-pictures_csp43198084.jpg => ../2.data/0.dataset/handsup/handsup12.jpg\n",
      "../2.data/0.dataset/handsup/happy-woman-hands-up-perfect-body-full-height-studio-white-background-beautiful-female-figure-31701913.jpg => ../2.data/0.dataset/handsup/handsup13.jpg\n",
      "../2.data/0.dataset/handsup/images (2).jpg => ../2.data/0.dataset/handsup/handsup14.jpg\n",
      "../2.data/0.dataset/handsup/images.jpg => ../2.data/0.dataset/handsup/handsup15.jpg\n",
      "../2.data/0.dataset/handsup/man-celebrating-with-both-hands-raised-picture_csp34978214.jpg => ../2.data/0.dataset/handsup/handsup16.jpg\n",
      "../2.data/0.dataset/handsup/portrait-happy-girl-sea-middle-aged-woman-swimsuit-standing-knee-deep-sea-water-shows-thumbs-up-both-hands-115144583.jpg => ../2.data/0.dataset/handsup/handsup17.jpg\n",
      "../2.data/0.dataset/handsup/pretty-casual-woman-celebrating-hands-air-pretty-casual-woman-celebrating-hands-air-standing-125239394.jpg => ../2.data/0.dataset/handsup/handsup18.jpg\n",
      "../2.data/0.dataset/handsup/put-your-hands-up-air-dance-me-emotive-happy-caucasian-woman-short-haircut-dancing-listening-put-your-hands-113570974.jpg => ../2.data/0.dataset/handsup/handsup19.jpg\n",
      "../2.data/0.dataset/handsup/stock-photo-young-girl-with-her-hands-up-in-the-air-splashing-13544.jpg => ../2.data/0.dataset/handsup/handsup20.jpg\n",
      "../2.data/0.dataset/handsup/woman-black-swimsuit-raising-hands-up-beautiful-young-curly-hair-standing-full-growth-wearing-her-isolated-white-49619859.jpg => ../2.data/0.dataset/handsup/handsup21.jpg\n",
      "../2.data/0.dataset/handsup/woman-raised-her-hands-up-beach-young-bikini-58380021.jpg => ../2.data/0.dataset/handsup/handsup22.jpg\n",
      "../2.data/0.dataset/handsup/woman-raising-her-hands-up-in-the-air-near-the-sea-ABKRHF.jpg => ../2.data/0.dataset/handsup/handsup23.jpg\n",
      "../2.data/0.dataset/handsup/young-woman-in-bikini-with-hands-up-DHPR1E.jpg => ../2.data/0.dataset/handsup/handsup24.jpg\n",
      "../2.data/0.dataset/meditation/0afbbd8c-3c4c-47db-ad71-d0eb10e559d1.jpg => ../2.data/0.dataset/meditation/meditation1.jpg\n",
      "../2.data/0.dataset/meditation/1-indian-young-girl-yoga-padmasana-meditation-exercise-in-park-JBBRRW.jpg => ../2.data/0.dataset/meditation/meditation2.jpg\n",
      "../2.data/0.dataset/meditation/102781628.2.jpg => ../2.data/0.dataset/meditation/meditation3.jpg\n",
      "../2.data/0.dataset/meditation/123488_50432_353.jpg => ../2.data/0.dataset/meditation/meditation4.jpg\n",
      "../2.data/0.dataset/meditation/19502395-fdsa.jpg => ../2.data/0.dataset/meditation/meditation5.jpg\n",
      "../2.data/0.dataset/meditation/1LHuumNQNOkMu5UgQJ2JBU.jpeg => ../2.data/0.dataset/meditation/meditation6.jpg\n",
      "../2.data/0.dataset/meditation/2225E04254E6EFDA16.png => ../2.data/0.dataset/meditation/meditation7.jpg\n",
      "../2.data/0.dataset/meditation/240599650.jpg => ../2.data/0.dataset/meditation/meditation8.jpg\n",
      "../2.data/0.dataset/meditation/69508987-fsa.jpg => ../2.data/0.dataset/meditation/meditation9.jpg\n",
      "../2.data/0.dataset/meditation/asdgas(1).jpg => ../2.data/0.dataset/meditation/meditation10.jpg\n",
      "../2.data/0.dataset/meditation/d6d78b22-38ea-45f7-ad65-17048eaca6df.jpg => ../2.data/0.dataset/meditation/meditation11.jpg\n",
      "../2.data/0.dataset/meditation/Gdsaf(2).jpg => ../2.data/0.dataset/meditation/meditation12.jpg\n",
      "../2.data/0.dataset/meditation/hqdefault.jpg => ../2.data/0.dataset/meditation/meditation13.jpg\n",
      "../2.data/0.dataset/meditation/htm_201310271036730103011.jpg => ../2.data/0.dataset/meditation/meditation14.jpg\n",
      "../2.data/0.dataset/meditation/images (1).jpg => ../2.data/0.dataset/meditation/meditation15.jpg\n",
      "../2.data/0.dataset/meditation/photo-handsome-indian-executive-meditating-27166605.jpg => ../2.data/0.dataset/meditation/meditation16.jpg\n",
      "../2.data/0.dataset/meditation/shutterstock_541377721_540 (1).jpg => ../2.data/0.dataset/meditation/meditation17.jpg\n",
      "../2.data/0.dataset/meditation/shutterstock_541377721_540.jpg => ../2.data/0.dataset/meditation/meditation18.jpg\n",
      "../2.data/0.dataset/meditation/shutterstock_570580195_540.jpg => ../2.data/0.dataset/meditation/meditation19.jpg\n",
      "../2.data/0.dataset/meditation/unnamed (1).jpg => ../2.data/0.dataset/meditation/meditation20.jpg\n",
      "../2.data/0.dataset/meditation/unnamed (2).jpg => ../2.data/0.dataset/meditation/meditation21.jpg\n",
      "../2.data/0.dataset/meditation/unnamed.jpg => ../2.data/0.dataset/meditation/meditation22.jpg\n",
      "../2.data/0.dataset/meditation/woman-meditating-at-home_23-2148108681.jpg => ../2.data/0.dataset/meditation/meditation23.jpg\n",
      "../2.data/0.dataset/meditation/woman-meditating-kr.jpg => ../2.data/0.dataset/meditation/meditation24.jpg\n",
      "../2.data/0.dataset/meditation/woman-meditating-on-yoga-mat-at-park-stock-image_csp59319866.jpg => ../2.data/0.dataset/meditation/meditation25.jpg\n",
      "../2.data/0.dataset/meditation/young-woman-in-lotus-position-stock-image__u34807563.jpg => ../2.data/0.dataset/meditation/meditation26.jpg\n",
      "../2.data/0.dataset/Siuuuu/18-185829_ronaldo-celebration-png-cristiano-ronaldo-siu-png-transparent.png => ../2.data/0.dataset/Siuuuu/Siuuuu1.jpg\n",
      "../2.data/0.dataset/Siuuuu/19eb1ca6b9f986c275736c458bad898f.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu2.jpg\n",
      "../2.data/0.dataset/Siuuuu/3f4e783f9917c38e9cce3112683665f6.png => ../2.data/0.dataset/Siuuuu/Siuuuu3.jpg\n",
      "../2.data/0.dataset/Siuuuu/7MlR3C.png => ../2.data/0.dataset/Siuuuu/Siuuuu4.jpg\n",
      "../2.data/0.dataset/Siuuuu/8365-list.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu5.jpg\n",
      "../2.data/0.dataset/Siuuuu/AGoJ5nTJyh3GstFFhpb5Rg.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu6.jpg\n",
      "../2.data/0.dataset/Siuuuu/b343781ad1ce0452ba6dcacdbd1e7608 (1).jpg => ../2.data/0.dataset/Siuuuu/Siuuuu7.jpg\n",
      "../2.data/0.dataset/Siuuuu/b343781ad1ce0452ba6dcacdbd1e7608.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu8.jpg\n",
      "../2.data/0.dataset/Siuuuu/c9438fd0eab28f6dbc2421f93b41dae7.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu9.jpg\n",
      "../2.data/0.dataset/Siuuuu/Cristiano-Ronaldo-in-real-madrid.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu10.jpg\n",
      "../2.data/0.dataset/Siuuuu/fcp,small,wall_texture,product,750x1000.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu11.jpg\n",
      "../2.data/0.dataset/Siuuuu/images (1).jpg => ../2.data/0.dataset/Siuuuu/Siuuuu12.jpg\n",
      "../2.data/0.dataset/Siuuuu/images (2).jpg => ../2.data/0.dataset/Siuuuu/Siuuuu13.jpg\n",
      "../2.data/0.dataset/Siuuuu/images (3).jpg => ../2.data/0.dataset/Siuuuu/Siuuuu14.jpg\n",
      "../2.data/0.dataset/Siuuuu/images.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu15.jpg\n",
      "../2.data/0.dataset/Siuuuu/Real+Madrid+v+Sevilla+FC+YL380_7-fvCx.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu16.jpg\n",
      "../2.data/0.dataset/Siuuuu/wp2271188.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu17.jpg\n",
      "../2.data/0.dataset/Siuuuu/wp2271192.jpg => ../2.data/0.dataset/Siuuuu/Siuuuu18.jpg\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "path = '../2.data/0.dataset/'\n",
    "path_list = ['crossedArm','handsup','meditation','Siuuuu']\n",
    "    \n",
    "def changeName(path, cName):\n",
    "    i = 1\n",
    "    for filename in os.listdir(path):\n",
    "        print(path+'/'+filename, '=>', path+'/'+str(cName)+str(i)+'.jpg')\n",
    "        os.rename(path+'/'+filename, path+'/'+str(cName)+str(i)+'.jpg')\n",
    "        i += 1\n",
    "\n",
    "for j in path_list:\n",
    "    changeName(path + j,j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv3 using Tensorflow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:09.142597Z",
     "start_time": "2020-11-18T11:32:01.726368Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2, os, glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Add, Concatenate, Conv2D,\n",
    "    Input, Lambda, LeakyReLU,\n",
    "    MaxPool2D, UpSampling2D, ZeroPadding2D\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import (\n",
    "    binary_crossentropy,\n",
    "    sparse_categorical_crossentropy\n",
    ")\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from yolo_utilities import *\n",
    "from yolo_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:09.172587Z",
     "start_time": "2020-11-18T11:32:09.160572Z"
    }
   },
   "outputs": [],
   "source": [
    "yolo_anchors = np.array([\n",
    "    (126, 146), (106, 258), (190, 160), (254, 133), (145, 254),\n",
    "    (200, 261), (306, 182), (174, 345), (284, 353)],\n",
    "    np.float32) / 416\n",
    "\n",
    "yolo_anchor_masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])\n",
    "\n",
    "yolo_tiny_anchors = np.array([\n",
    "    (10, 14), (23, 27), (37, 58),\n",
    "    (81, 82), (135, 169), (344, 319)],\n",
    "    np.float32) / 416\n",
    "\n",
    "yolo_tiny_anchor_masks = np.array([[3, 4, 5], [0, 1, 2]])\n",
    "\n",
    "class_names = ['crossedArm', 'handsup', 'meditation', 'Siuuuu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:09.217523Z",
     "start_time": "2020-11-18T11:32:09.191557Z"
    }
   },
   "outputs": [],
   "source": [
    "def YoloV3(size=None, channels=3, anchors=yolo_anchors, masks=yolo_anchor_masks, classes=4, training=False):\n",
    "    # 이미지 사이즈와 채널 맞추어서 input데이터로 입력\n",
    "    x = inputs = Input([size, size, channels])\n",
    "    x_36, x_61, x = Darknet(name='yolo_darknet')(x)\n",
    "    x = YoloConv(512, name='yolo_conv_0')(x)\n",
    "    output_0 = YoloOutput(512, len(masks[0]), classes, name='yolo_output_0')(x)\n",
    "    x = YoloConv(256, name='yolo_conv_1')((x, x_61))\n",
    "    output_1 = YoloOutput(256, len(masks[1]), classes, name='yolo_output_1')(x)\n",
    "    x = YoloConv(128, name='yolo_conv_2')((x, x_36))\n",
    "    output_2 = YoloOutput(128, len(masks[2]), classes, name='yolo_output_2')(x)\n",
    "    if training:\n",
    "        return Model(inputs, (output_0, output_1, output_2), name='yolov3')\n",
    "        # True로 잡을시에 predict 아웃풋값이 3개만 나옴\n",
    "    boxes_0 = Lambda(lambda x: yolo_boxes(x, anchors[masks[0]], classes),\n",
    "                     name='yolo_boxes_0')(output_0)\n",
    "    boxes_1 = Lambda(lambda x: yolo_boxes(x, anchors[masks[1]], classes),\n",
    "                     name='yolo_boxes_1')(output_1)\n",
    "    boxes_2 = Lambda(lambda x: yolo_boxes(x, anchors[masks[2]], classes),\n",
    "                     name='yolo_boxes_2')(output_2)\n",
    "    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\n",
    "                     name='yolo_nms')((boxes_0[:3], boxes_1[:3], boxes_2[:3]))\n",
    "    return Model(inputs, outputs, name='yolov3')\n",
    "            # inputs : 사이즈와 채널수\n",
    "            # outputs : \n",
    "\n",
    "\n",
    "def YoloV3Tiny(size=None, channels=3, anchors=yolo_tiny_anchors, masks=yolo_tiny_anchor_masks, classes=80, training=False):\n",
    "    x = inputs = Input([size, size, channels])\n",
    "    x_8, x = DarknetTiny(name='yolo_darknet')(x)\n",
    "    x = YoloConvTiny(256, name='yolo_conv_0')(x)\n",
    "    output_0 = YoloOutput(256, len(masks[0]), classes, name='yolo_output_0')(x)\n",
    "    x = YoloConvTiny(128, name='yolo_conv_1')((x, x_8))\n",
    "    output_1 = YoloOutput(128, len(masks[1]), classes, name='yolo_output_1')(x)\n",
    "    if training:\n",
    "        return Model(inputs, (output_0, output_1), name='yolov3')\n",
    "    boxes_0 = Lambda(lambda x: yolo_boxes(x, anchors[masks[0]], classes),\n",
    "                     name='yolo_boxes_0')(output_0)\n",
    "    boxes_1 = Lambda(lambda x: yolo_boxes(x, anchors[masks[1]], classes),\n",
    "                     name='yolo_boxes_1')(output_1)\n",
    "    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\n",
    "                     name='yolo_nms')((boxes_0[:3], boxes_1[:3]))\n",
    "    return Model(inputs, outputs, name='yolov3_tiny')\n",
    "\n",
    "\n",
    "def YoloLoss(anchors, classes=4, ignore_thresh=0.5):\n",
    "    def yolo_loss(y_true, y_pred):\n",
    "        # 1. transform all pred outputs\n",
    "        # y_pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...cls))\n",
    "        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(y_pred, anchors, classes)\n",
    "        pred_xy = pred_xywh[..., 0:2]\n",
    "        pred_wh = pred_xywh[..., 2:4]\n",
    "        # 2. transform all true outputs\n",
    "        # y_true: (batch_size, grid, grid, anchors, (x1, y1, x2, y2, obj, cls))\n",
    "        true_box, true_obj, true_class_idx = tf.split(\n",
    "            y_true, (4, 1, 1), axis=-1)\n",
    "        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) / 2\n",
    "        true_wh = true_box[..., 2:4] - true_box[..., 0:2]\n",
    "        # give higher weights to small boxes\n",
    "        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n",
    "        # 3. inverting the pred box equations\n",
    "        grid_size = tf.shape(y_true)[1]\n",
    "        grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "        grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n",
    "        true_xy = true_xy * tf.cast(grid_size, tf.float32) - \\\n",
    "            tf.cast(grid, tf.float32)\n",
    "        true_wh = tf.math.log(true_wh / anchors)\n",
    "        true_wh = tf.where(tf.math.is_inf(true_wh), tf.zeros_like(true_wh), true_wh)\n",
    "        # 4. calculate all masks\n",
    "        obj_mask = tf.squeeze(true_obj, -1)\n",
    "        # ignore false positive when iou is over threshold\n",
    "        true_box_flat = tf.boolean_mask(true_box, tf.cast(obj_mask, tf.bool))\n",
    "        best_iou = tf.reduce_max(broadcast_iou(\n",
    "            pred_box, true_box_flat), axis=-1)\n",
    "        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n",
    "        # 5. calculate all losses\n",
    "        xy_loss = obj_mask * box_loss_scale * \\\n",
    "            tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n",
    "        wh_loss = obj_mask * box_loss_scale * \\\n",
    "            tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n",
    "        obj_loss = binary_crossentropy(true_obj, pred_obj)\n",
    "        obj_loss = obj_mask * obj_loss + \\\n",
    "            (1 - obj_mask) * ignore_mask * obj_loss\n",
    "        # Could also use binary_crossentropy instead\n",
    "        class_loss = obj_mask * sparse_categorical_crossentropy(\n",
    "            true_class_idx, pred_class)\n",
    "        # 6. sum over (batch, gridx, gridy, anchors) => (batch, 1)\n",
    "        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3))\n",
    "        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3))\n",
    "        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3))\n",
    "        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3))\n",
    "        return xy_loss + wh_loss + obj_loss + class_loss\n",
    "    return yolo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Weight load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:13.995304Z",
     "start_time": "2020-11-18T11:32:09.236559Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"yolov3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_darknet (Functional)       ((None, None, None,  40620640    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_0 (Functional)        (None, None, None, 5 11024384    yolo_darknet[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_1 (Functional)        (None, None, None, 2 2957312     yolo_conv_0[0][0]                \n",
      "                                                                 yolo_darknet[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_2 (Functional)        (None, None, None, 1 741376      yolo_conv_1[0][0]                \n",
      "                                                                 yolo_darknet[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_0 (Functional)      (None, None, None, 3 4750363     yolo_conv_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_1 (Functional)      (None, None, None, 3 1195547     yolo_conv_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_2 (Functional)      (None, None, None, 3 302875      yolo_conv_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_nms (Lambda)               ((None, 100, 4), (No 0           yolo_boxes_0[0][0]               \n",
      "                                                                 yolo_boxes_0[0][1]               \n",
      "                                                                 yolo_boxes_0[0][2]               \n",
      "                                                                 yolo_boxes_1[0][0]               \n",
      "                                                                 yolo_boxes_1[0][1]               \n",
      "                                                                 yolo_boxes_1[0][2]               \n",
      "                                                                 yolo_boxes_2[0][0]               \n",
      "                                                                 yolo_boxes_2[0][1]               \n",
      "                                                                 yolo_boxes_2[0][2]               \n",
      "==================================================================================================\n",
      "Total params: 61,592,497\n",
      "Trainable params: 61,539,889\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "yolo = YoloV3(classes = 4)\n",
    "\n",
    "yolo.summary()\n",
    "# plot_model(\n",
    "#     yolo, rankdir = 'TB',\n",
    "#     to_file = 'yolo_model.png',\n",
    "#     show_shapes = False,\n",
    "#     show_layer_names = True,\n",
    "#     expand_nested = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:15.854642Z",
     "start_time": "2020-11-18T11:32:14.011308Z"
    }
   },
   "outputs": [],
   "source": [
    "# yolo weight를 로드해주는 코드\n",
    "# 모델, weight 경로, Tiny 여부\n",
    "\n",
    "load_darknet_weights(yolo, './model&weight/yolov3/yolov3_final.weights', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:15.884707Z",
     "start_time": "2020-11-18T11:32:15.870689Z"
    }
   },
   "outputs": [],
   "source": [
    "# 인풋이미지를 디택팅 이후 그림을 그려주는 함수\n",
    "\n",
    "# 이미지 인풋하기\n",
    "# image_file : 파일 경로 및 이름 넣기\n",
    "# visualize : 화면에 띄우기 여부\n",
    "# figsize : 그림 크기\n",
    "\n",
    "def predict(image_file, visualize = True, figsize = (16, 16)):\n",
    "    img = tf.image.decode_image(open(image_file, 'rb').read(), channels=3)\n",
    "    \n",
    "    img = tf.expand_dims(img, 0)\n",
    "    img = transform_images(img, 416)\n",
    "    boxes, scores, classes, nums = yolo.predict(img)\n",
    "    img = cv2.cvtColor(cv2.imread(image_file), cv2.COLOR_BGR2RGB)\n",
    "    img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n",
    "    if visualize:\n",
    "        fig, axes = plt.subplots(figsize = figsize)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    return boxes, scores, classes, nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPS 측정\n",
    "start = time.time()\n",
    "boxes, scores, classes, nums = predict('../2.data/0.dataset/crossedArm/crossedArm5.jpg',figsize = (20, 20))\n",
    "print('time :', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:32:15.914996Z",
     "start_time": "2020-11-18T11:32:15.900823Z"
    }
   },
   "outputs": [],
   "source": [
    "# 저장된 비디오에 Yolo를 수행하는 함수\n",
    "\n",
    "def showVideo(input_source):\n",
    "    cap = cv2.VideoCapture(input_source)\n",
    "    prev_time = 0\n",
    "    \n",
    "    snpNum = np.zeros(4)\n",
    "    while True:\n",
    "        ret, frame = cap.read() # 재생되는 비디오를 한 프레임씩 읽기\n",
    "        #frame = cv2.flip(frame, 1) # 제대로 읽을 시에 ret : True or False, frame : 읽은 프레임\n",
    "\n",
    "        if not ret:\n",
    "            print('비디오 읽기 오류')\n",
    "            break\n",
    "\n",
    "        framePose = pose_estimate(frame)\n",
    "        img = tf.constant(framePose)\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        img = transform_images(img, 416)\n",
    "        boxes, scores, classes, nums = yolo.predict(img)\n",
    "\n",
    "        img = draw_outputs(framePose, (boxes, scores, classes, nums), class_names)\n",
    "        img = cv2.resize(img, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "        #boxes, scores, classes, nums, img = predict(frame,figsize = (20, 20))\n",
    "        \n",
    "        if scores[0][0] > 0.95:\n",
    "            if classes[0][0] == 0:\n",
    "                snpNum[0] += 1\n",
    "                cv2.imwrite('./snap/crossedArm/crossedArm{}.jpg'.format(int(snpNum[0])), img)\n",
    "            elif classes[0][0] == 1:\n",
    "                snpNum[1] += 1\n",
    "                cv2.imwrite('./snap/handsup/handsup{}.jpg'.format(int(snpNum[1])), img)\n",
    "            elif classes[0][0] == 3:\n",
    "                snpNum[3] += 1\n",
    "                cv2.imwrite('./snap/Siuuuu/Siuuuu{}.jpg'.format(int(snpNum[3])), img)\n",
    "\n",
    "                \n",
    "\n",
    "        cv2.imshow('video', img) # 변환한 프레임을 화면에 디스플레이\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == 27: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:35:26.596384Z",
     "start_time": "2020-11-18T11:32:19.141217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비디오 읽기 오류\n"
     ]
    }
   ],
   "source": [
    "showVideo('./images/testVideo.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T11:18:36.045907Z",
     "start_time": "2020-11-18T11:18:36.026941Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 웹캠를 구동하여 pose estimation을 수행하는 함수\n",
    "\n",
    "def showWebcamYOLO(FPS):\n",
    "\n",
    "    try:\n",
    "        print('카메라를 구동합니다.')\n",
    "        cap = cv2.VideoCapture(0) # VideoCapture 객체 생성 -> 구동할 장치 번호 입력(캠이 하나임으로 0번)\n",
    "    except:                      # 만약 캠구동이 아니라 저장된 파일을 재생하기 원할 시, 경로와 파일이름 입력\n",
    "        print('카메라 구동 실패')\n",
    "        return\n",
    "\n",
    "    cap.set(3, 480) # 프레임 크기 설정\n",
    "    cap.set(4, 320)    \n",
    "    prev_time = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read() # 재생되는 비디오를 한 프레임씩 읽기 \n",
    "        frame = cv2.flip(frame, 1) # 제대로 읽을 시에 ret : True or False, frame : 읽은 프레임\n",
    "        current_time = time.time() - prev_time\n",
    "        \n",
    "        if not ret:\n",
    "            print('비디오 읽기 오류')\n",
    "            break\n",
    "\n",
    "        img = tf.constant(frame)\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        img = transform_images(img, 416)\n",
    "        boxes, scores, classes, nums = yolo.predict(img)\n",
    "\n",
    "        img = draw_outputs(frame, (boxes, scores, classes, nums), class_names)\n",
    "        #img = cv2.resize(img, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)        \n",
    "        \n",
    "        if (ret is True) and (current_time > 1./ FPS) :\n",
    "            prev_time = time.time()\n",
    "            cv2.imshow('video', img) # 변환한 프레임을 화면에 디스플레이\n",
    "        \n",
    "        \n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            print('카메라 구동을 종료합니다.')\n",
    "            break\n",
    "    \n",
    "    cap.release() # 오픈한 cap 객체 해제 **** 필수 ****\n",
    "    cv2.destroyAllWindows() # 윈도우 창 답기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T03:50:48.530510Z",
     "start_time": "2020-11-18T03:50:48.519550Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 웹캠를 구동하여 pose estimation을 수행하는 함수\n",
    "\n",
    "def SnapWebcam(FPS):\n",
    "\n",
    "    try:\n",
    "        print('카메라를 구동합니다.')\n",
    "        cap = cv2.VideoCapture(1) # VideoCapture 객체 생성 -> 구동할 장치 번호 입력(캠이 하나임으로 0번)\n",
    "    except:                      # 만약 캠구동이 아니라 저장된 파일을 재생하기 원할 시, 경로와 파일이름 입력\n",
    "        print('카메라 구동 실패')\n",
    "        return\n",
    "\n",
    "    cap.set(3, 480) # 프레임 크기 설정\n",
    "    cap.set(4, 320)    \n",
    "    prev_time = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read() # 재생되는 비디오를 한 프레임씩 읽기 \n",
    "        frame = cv2.flip(frame, 1) # 제대로 읽을 시에 ret : True or False, frame : 읽은 프레임\n",
    "        current_time = time.time() - prev_time\n",
    "        \n",
    "        if not ret:\n",
    "            print('비디오 읽기 오류')\n",
    "            break\n",
    "        \n",
    "        framePose = pose_estimate(frame)\n",
    "        img = tf.constant(framePose)\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        img = transform_images(img, 416)\n",
    "        boxes, scores, classes, nums = yolo.predict(img)\n",
    "        img = draw_outputs(framePose, (boxes, scores, classes, nums), class_names)\n",
    "        \n",
    "        snpNum = np.zeros(4)\n",
    "        if scores[0][0] > 0.9:\n",
    "            if classes[0][0] == 0:\n",
    "                snpNum[0] += 1\n",
    "                cv2.imwrite('./snap/crossedArm/crossedArm{}.jpg'.format(snpNum[0]), img)\n",
    "            elif classes[0][0] == 1:\n",
    "                snpNum[1] += 1\n",
    "                cv2.imwrite('./snap/handsup/handsup{}.jpg'.format(snpNum[1]), img)\n",
    "            elif classes[0][0] == 2:\n",
    "                snpNum[2] += 1\n",
    "                cv2.imwrite('./snap/meditation/meditation{}.jpg'.format(snpNum[2]), img)\n",
    "            elif classes[0][0] == 3:\n",
    "                snpNum[3] += 1\n",
    "                cv2.imwrite('./snap/Siuuuu/Siuuuu{}.jpg'.format(snpNum[3]), img)\n",
    "        \n",
    "        if (ret is True) and (current_time > 1./ FPS) :\n",
    "            prev_time = time.time()\n",
    "            cv2.imshow('video', img) # 변환한 프레임을 화면에 디스플레이\n",
    "        \n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            print('카메라 구동을 종료합니다.')\n",
    "            break\n",
    "    \n",
    "    cap.release() # 오픈한 cap 객체 해제 **** 필수 ****\n",
    "    cv2.destroyAllWindows() # 윈도우 창 답기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-18T11:18:37.866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카메라를 구동합니다.\n"
     ]
    }
   ],
   "source": [
    "showWebcamYOLO(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비디오 읽기 오류\n"
     ]
    }
   ],
   "source": [
    "showVideo('./images/WIN_20201118_13_16_32_Pro.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
